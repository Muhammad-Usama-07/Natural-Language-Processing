{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Usama-07/Natural-Language-Processing/blob/main/NLP_Basics/workFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dcdbb8c",
      "metadata": {
        "id": "2dcdbb8c"
      },
      "source": [
        "# Regular Expressions & Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d65c3f",
      "metadata": {
        "id": "c8d65c3f"
      },
      "source": [
        "### Using Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fd2532",
      "metadata": {
        "id": "f5fd2532"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26c7b20",
      "metadata": {
        "id": "d26c7b20"
      },
      "source": [
        "### Seperate Sentence on the Basis of sentence endings (. ? !)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb224dd",
      "metadata": {
        "id": "edb224dd",
        "outputId": "f46f34d3-8798-4978-e21a-19d8cb21069e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog', '\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate', '']\n"
          ]
        }
      ],
      "source": [
        "sentence = 'In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate.'\n",
        "pattern_of_sentence_endings = r\"[.?!]\"\n",
        "print(re.split(pattern_of_sentence_endings, sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec190c5a",
      "metadata": {
        "id": "ec190c5a"
      },
      "source": [
        "### Seperate Words from String"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b7d79d",
      "metadata": {
        "id": "f1b7d79d",
        "outputId": "e0ace6dd-0439-4893-fddb-a66184e6ad47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['In', 'linguistics', 'and', 'grammar,', 'a', 'sentence', 'is', 'a', 'linguistic', 'expression']\n"
          ]
        }
      ],
      "source": [
        "sentence = 'In linguistics and grammar, a sentence is a linguistic expression'\n",
        "pattern_of_word_seperation = r\"\\s+\"\n",
        "print(re.split(pattern_of_word_seperation, sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4893e21",
      "metadata": {
        "id": "d4893e21"
      },
      "source": [
        "### Find all capitalized words from String"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e60384",
      "metadata": {
        "id": "f3e60384",
        "outputId": "7ba4aab6-c736-4d1d-adf8-514b48a56a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['In', 'Linguistic', 'Expression']\n"
          ]
        }
      ],
      "source": [
        "sentence = 'In linguistics and grammar, a sentence is a Linguistic Expression'\n",
        "capitalized_words_pattern = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words_pattern, sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c690d8a2",
      "metadata": {
        "id": "c690d8a2"
      },
      "source": [
        "### Find all Digits from String"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4db9da",
      "metadata": {
        "id": "ea4db9da",
        "outputId": "2c18eb11-c8aa-4b25-e7e9-29911cbce32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['2', '123']\n"
          ]
        }
      ],
      "source": [
        "sentence = 'In linguistics and grammar, a 2 sentence is a linguistic expression, 123'\n",
        "find_digit_pattern = r\"\\d+\"\n",
        "print(re.findall(find_digit_pattern, sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search for a first and last occurrence of word  "
      ],
      "metadata": {
        "id": "tvnM7r0ryda2"
      },
      "id": "tvnM7r0ryda2"
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate.'\n",
        "result = re.search(\"and\", paragraph)\n",
        "print(result.start(), result.end())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7QNRNK7yx03",
        "outputId": "b42a163b-86e5-499e-c76f-bb0cb32c057d"
      },
      "id": "m7QNRNK7yx03",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Regex with NLTK Tokenization"
      ],
      "metadata": {
        "id": "CPI1FZHA277N"
      },
      "id": "CPI1FZHA277N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Find hashtag from Paragraph"
      ],
      "metadata": {
        "id": "15K_1TiF4Srm"
      },
      "id": "15K_1TiF4Srm"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate. #dummytext #hashtagcheck'\n",
        "find_hashtag_pattern = r\"#\\w+\"\n",
        "hashtag_words = regexp_tokenize(paragraph, find_hashtag_pattern)\n",
        "print(hashtag_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mjqzbnQ4agj",
        "outputId": "7048cb1c-7e7c-42bf-94e8-adb872db5d95"
      },
      "id": "6mjqzbnQ4agj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#dummytext', '#hashtagcheck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Find both hashtag and mentions from Paragraph"
      ],
      "metadata": {
        "id": "mwauFr2b5z_x"
      },
      "id": "mwauFr2b5z_x"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate. #dummytext #hashtagcheck @today'\n",
        "find_hashtag_pattern = r\"([\\@\\#]\\w+)\"\n",
        "hashtag_words = regexp_tokenize(paragraph, find_hashtag_pattern)\n",
        "print(hashtag_words)"
      ],
      "metadata": {
        "id": "VhAfSFOX53la",
        "outputId": "f28ce0ce-3cc8-4c7f-96df-34b6d63a1f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VhAfSFOX53la",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#dummytext', '#hashtagcheck', '@today']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize Tweets"
      ],
      "metadata": {
        "id": "k2My2-Dq6dU5"
      },
      "id": "k2My2-Dq6dU5"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "paragraph = ['In linguistics and grammar, a sentence is a linguistic expression', 'such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate. #dummytext #hashtagcheck @today']\n",
        "tknzr_object = TweetTokenizer()\n",
        "all_tokens = [tknzr_object.tokenize(t) for t in paragraph]\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "id": "2kxVXetv6ihj",
        "outputId": "08603b60-65fb-4c5a-c96a-a730da8c6e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2kxVXetv6ihj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['In', 'linguistics', 'and', 'grammar', ',', 'a', 'sentence', 'is', 'a', 'linguistic', 'expression'], ['such', 'as', 'the', 'English', 'example', '\"', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', '\"', 'In', 'traditional', 'grammar', ',', 'it', 'is', 'typically', 'defined', 'as', 'a', 'string', 'of', 'words', 'that', 'expresses', 'a', 'complete', 'thought', ',', 'or', 'as', 'a', 'unit', 'consisting', 'of', 'a', 'subject', 'and', 'predicate', '.', '#dummytext', '#hashtagcheck', '@today']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb9b508",
      "metadata": {
        "id": "aeb9b508"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21cb2ca7",
      "metadata": {
        "id": "21cb2ca7"
      },
      "source": [
        "### Using Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82deaaed",
      "metadata": {
        "id": "82deaaed"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f8229c",
      "metadata": {
        "id": "63f8229c"
      },
      "source": [
        "### Spliting Paragraph into Sentences "
      ]
    },
    {
      "cell_type": "raw",
      "id": "a6fd9073",
      "metadata": {
        "id": "a6fd9073"
      },
      "source": [
        "To resolve error in doing sent_tokenize use:\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e6459f6",
      "metadata": {
        "id": "4e6459f6",
        "outputId": "c581f3dd-0f33-4fbe-9346-fa0bf9f88cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\"', 'In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate.']\n"
          ]
        }
      ],
      "source": [
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate.'\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtering tokens"
      ],
      "metadata": {
        "id": "B5Ic3RyaRDPW"
      },
      "id": "B5Ic3RyaRDPW"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression, such as the 1 English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought #this, or @company as a unit consisting of a subject and predicate.'\n",
        "word_tokens = word_tokenize(paragraph)\n",
        "letters_tokens = [a_word for a_word in word_tokens if a_word.isalpha()]\n",
        "print('letters tokens: ', letters_tokens)\n",
        "\n",
        "digit_tokens = [a_word for a_word in word_tokens if a_word.isdigit()]\n",
        "print('digit tokens: ', digit_tokens)\n",
        "\n",
        "digit_letters_tokens = [a_word for a_word in word_tokens if a_word.isalnum()]\n",
        "print('letter and digit tokens: ', digit_letters_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX2zhsTORHgI",
        "outputId": "5bfeb4e2-a71b-4a97-9df6-1b8d1112d121"
      },
      "id": "UX2zhsTORHgI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "letters tokens:  ['In', 'linguistics', 'and', 'grammar', 'a', 'sentence', 'is', 'a', 'linguistic', 'expression', 'such', 'as', 'the', 'English', 'example', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'In', 'traditional', 'grammar', 'it', 'is', 'typically', 'defined', 'as', 'a', 'string', 'of', 'words', 'that', 'expresses', 'a', 'complete', 'thought', 'this', 'or', 'company', 'as', 'a', 'unit', 'consisting', 'of', 'a', 'subject', 'and', 'predicate']\n",
            "digit tokens:  ['1']\n",
            "letter and digit tokens:  ['In', 'linguistics', 'and', 'grammar', 'a', 'sentence', 'is', 'a', 'linguistic', 'expression', 'such', 'as', 'the', '1', 'English', 'example', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'In', 'traditional', 'grammar', 'it', 'is', 'typically', 'defined', 'as', 'a', 'string', 'of', 'words', 'that', 'expresses', 'a', 'complete', 'thought', 'this', 'or', 'company', 'as', 'a', 'unit', 'consisting', 'of', 'a', 'subject', 'and', 'predicate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find emoji only"
      ],
      "metadata": {
        "id": "ej2E0V8xuAV6"
      },
      "id": "ej2E0V8xuAV6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3243ef",
      "metadata": {
        "id": "5a3243ef",
        "outputId": "01605a24-541a-485a-860f-01e3c7807f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['üöï', 'üçï']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "st = 'I like driving üöï and eating üçï'\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(st, emoji))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Identification"
      ],
      "metadata": {
        "id": "legRRtsEwEDv"
      },
      "id": "legRRtsEwEDv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Counter with bag-of-words"
      ],
      "metadata": {
        "id": "fOFjnu-SEnWm"
      },
      "id": "fOFjnu-SEnWm"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate.'\n",
        "tokens = word_tokenize(paragraph)\n",
        "tokens = [t for t in tokens]\n",
        "bow_simple = Counter(tokens)\n",
        "print(bow_simple.most_common(10))\n"
      ],
      "metadata": {
        "id": "6CNQdvUfuodn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7f2d5c-3f79-4a50-9b39-b5e94604a3f3"
      },
      "id": "6CNQdvUfuodn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 6), (',', 4), ('as', 3), ('In', 2), ('and', 2), ('grammar', 2), ('is', 2), ('the', 2), ('.', 2), ('of', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Default English Stopwords"
      ],
      "metadata": {
        "id": "xWPw0JXlZyao"
      },
      "id": "xWPw0JXlZyao"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "ENGLISH_STOP_WORDS"
      ],
      "metadata": {
        "id": "bql02Zv_Z1Ey",
        "outputId": "dfbfce04-9360-437d-c1f3-cf6d0a57b3bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bql02Zv_Z1Ey",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "frozenset({'a',\n",
              "           'about',\n",
              "           'above',\n",
              "           'across',\n",
              "           'after',\n",
              "           'afterwards',\n",
              "           'again',\n",
              "           'against',\n",
              "           'all',\n",
              "           'almost',\n",
              "           'alone',\n",
              "           'along',\n",
              "           'already',\n",
              "           'also',\n",
              "           'although',\n",
              "           'always',\n",
              "           'am',\n",
              "           'among',\n",
              "           'amongst',\n",
              "           'amoungst',\n",
              "           'amount',\n",
              "           'an',\n",
              "           'and',\n",
              "           'another',\n",
              "           'any',\n",
              "           'anyhow',\n",
              "           'anyone',\n",
              "           'anything',\n",
              "           'anyway',\n",
              "           'anywhere',\n",
              "           'are',\n",
              "           'around',\n",
              "           'as',\n",
              "           'at',\n",
              "           'back',\n",
              "           'be',\n",
              "           'became',\n",
              "           'because',\n",
              "           'become',\n",
              "           'becomes',\n",
              "           'becoming',\n",
              "           'been',\n",
              "           'before',\n",
              "           'beforehand',\n",
              "           'behind',\n",
              "           'being',\n",
              "           'below',\n",
              "           'beside',\n",
              "           'besides',\n",
              "           'between',\n",
              "           'beyond',\n",
              "           'bill',\n",
              "           'both',\n",
              "           'bottom',\n",
              "           'but',\n",
              "           'by',\n",
              "           'call',\n",
              "           'can',\n",
              "           'cannot',\n",
              "           'cant',\n",
              "           'co',\n",
              "           'con',\n",
              "           'could',\n",
              "           'couldnt',\n",
              "           'cry',\n",
              "           'de',\n",
              "           'describe',\n",
              "           'detail',\n",
              "           'do',\n",
              "           'done',\n",
              "           'down',\n",
              "           'due',\n",
              "           'during',\n",
              "           'each',\n",
              "           'eg',\n",
              "           'eight',\n",
              "           'either',\n",
              "           'eleven',\n",
              "           'else',\n",
              "           'elsewhere',\n",
              "           'empty',\n",
              "           'enough',\n",
              "           'etc',\n",
              "           'even',\n",
              "           'ever',\n",
              "           'every',\n",
              "           'everyone',\n",
              "           'everything',\n",
              "           'everywhere',\n",
              "           'except',\n",
              "           'few',\n",
              "           'fifteen',\n",
              "           'fifty',\n",
              "           'fill',\n",
              "           'find',\n",
              "           'fire',\n",
              "           'first',\n",
              "           'five',\n",
              "           'for',\n",
              "           'former',\n",
              "           'formerly',\n",
              "           'forty',\n",
              "           'found',\n",
              "           'four',\n",
              "           'from',\n",
              "           'front',\n",
              "           'full',\n",
              "           'further',\n",
              "           'get',\n",
              "           'give',\n",
              "           'go',\n",
              "           'had',\n",
              "           'has',\n",
              "           'hasnt',\n",
              "           'have',\n",
              "           'he',\n",
              "           'hence',\n",
              "           'her',\n",
              "           'here',\n",
              "           'hereafter',\n",
              "           'hereby',\n",
              "           'herein',\n",
              "           'hereupon',\n",
              "           'hers',\n",
              "           'herself',\n",
              "           'him',\n",
              "           'himself',\n",
              "           'his',\n",
              "           'how',\n",
              "           'however',\n",
              "           'hundred',\n",
              "           'i',\n",
              "           'ie',\n",
              "           'if',\n",
              "           'in',\n",
              "           'inc',\n",
              "           'indeed',\n",
              "           'interest',\n",
              "           'into',\n",
              "           'is',\n",
              "           'it',\n",
              "           'its',\n",
              "           'itself',\n",
              "           'keep',\n",
              "           'last',\n",
              "           'latter',\n",
              "           'latterly',\n",
              "           'least',\n",
              "           'less',\n",
              "           'ltd',\n",
              "           'made',\n",
              "           'many',\n",
              "           'may',\n",
              "           'me',\n",
              "           'meanwhile',\n",
              "           'might',\n",
              "           'mill',\n",
              "           'mine',\n",
              "           'more',\n",
              "           'moreover',\n",
              "           'most',\n",
              "           'mostly',\n",
              "           'move',\n",
              "           'much',\n",
              "           'must',\n",
              "           'my',\n",
              "           'myself',\n",
              "           'name',\n",
              "           'namely',\n",
              "           'neither',\n",
              "           'never',\n",
              "           'nevertheless',\n",
              "           'next',\n",
              "           'nine',\n",
              "           'no',\n",
              "           'nobody',\n",
              "           'none',\n",
              "           'noone',\n",
              "           'nor',\n",
              "           'not',\n",
              "           'nothing',\n",
              "           'now',\n",
              "           'nowhere',\n",
              "           'of',\n",
              "           'off',\n",
              "           'often',\n",
              "           'on',\n",
              "           'once',\n",
              "           'one',\n",
              "           'only',\n",
              "           'onto',\n",
              "           'or',\n",
              "           'other',\n",
              "           'others',\n",
              "           'otherwise',\n",
              "           'our',\n",
              "           'ours',\n",
              "           'ourselves',\n",
              "           'out',\n",
              "           'over',\n",
              "           'own',\n",
              "           'part',\n",
              "           'per',\n",
              "           'perhaps',\n",
              "           'please',\n",
              "           'put',\n",
              "           'rather',\n",
              "           're',\n",
              "           'same',\n",
              "           'see',\n",
              "           'seem',\n",
              "           'seemed',\n",
              "           'seeming',\n",
              "           'seems',\n",
              "           'serious',\n",
              "           'several',\n",
              "           'she',\n",
              "           'should',\n",
              "           'show',\n",
              "           'side',\n",
              "           'since',\n",
              "           'sincere',\n",
              "           'six',\n",
              "           'sixty',\n",
              "           'so',\n",
              "           'some',\n",
              "           'somehow',\n",
              "           'someone',\n",
              "           'something',\n",
              "           'sometime',\n",
              "           'sometimes',\n",
              "           'somewhere',\n",
              "           'still',\n",
              "           'such',\n",
              "           'system',\n",
              "           'take',\n",
              "           'ten',\n",
              "           'than',\n",
              "           'that',\n",
              "           'the',\n",
              "           'their',\n",
              "           'them',\n",
              "           'themselves',\n",
              "           'then',\n",
              "           'thence',\n",
              "           'there',\n",
              "           'thereafter',\n",
              "           'thereby',\n",
              "           'therefore',\n",
              "           'therein',\n",
              "           'thereupon',\n",
              "           'these',\n",
              "           'they',\n",
              "           'thick',\n",
              "           'thin',\n",
              "           'third',\n",
              "           'this',\n",
              "           'those',\n",
              "           'though',\n",
              "           'three',\n",
              "           'through',\n",
              "           'throughout',\n",
              "           'thru',\n",
              "           'thus',\n",
              "           'to',\n",
              "           'together',\n",
              "           'too',\n",
              "           'top',\n",
              "           'toward',\n",
              "           'towards',\n",
              "           'twelve',\n",
              "           'twenty',\n",
              "           'two',\n",
              "           'un',\n",
              "           'under',\n",
              "           'until',\n",
              "           'up',\n",
              "           'upon',\n",
              "           'us',\n",
              "           'very',\n",
              "           'via',\n",
              "           'was',\n",
              "           'we',\n",
              "           'well',\n",
              "           'were',\n",
              "           'what',\n",
              "           'whatever',\n",
              "           'when',\n",
              "           'whence',\n",
              "           'whenever',\n",
              "           'where',\n",
              "           'whereafter',\n",
              "           'whereas',\n",
              "           'whereby',\n",
              "           'wherein',\n",
              "           'whereupon',\n",
              "           'wherever',\n",
              "           'whether',\n",
              "           'which',\n",
              "           'while',\n",
              "           'whither',\n",
              "           'who',\n",
              "           'whoever',\n",
              "           'whole',\n",
              "           'whom',\n",
              "           'whose',\n",
              "           'why',\n",
              "           'will',\n",
              "           'with',\n",
              "           'within',\n",
              "           'without',\n",
              "           'would',\n",
              "           'yet',\n",
              "           'you',\n",
              "           'your',\n",
              "           'yours',\n",
              "           'yourself',\n",
              "           'yourselves'})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detect Language"
      ],
      "metadata": {
        "id": "0DQ9uagWfmCz"
      },
      "id": "0DQ9uagWfmCz"
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect_langs\n",
        "st = 'ÁªùÂØπ‰∏çÊòØ„ÄÇ' # chinese\n",
        "print(detect_langs(st))"
      ],
      "metadata": {
        "id": "33LXUwjWZ4Af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa79c76-6a76-4a80-818e-f3f8827ced66"
      },
      "id": "33LXUwjWZ4Af",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[zh-cn:0.999997321572072]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Text Processing using Lemmatization"
      ],
      "metadata": {
        "id": "aRm1Vm7Ng7p3"
      },
      "id": "aRm1Vm7Ng7p3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "\n",
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression, such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate.'\n",
        "tokens = word_tokenize(paragraph)\n",
        "tokens = [t for t in tokens]\n",
        "bow_simple = Counter(tokens)\n",
        "\n",
        "alpha_only = [t for t in tokens if t.lower().isalpha()]\n",
        "\n",
        "stopswords_removed = [t for t in alpha_only if t not in ENGLISH_STOP_WORDS]\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_lokens = [wordnet_lemmatizer.lemmatize(t) for t in stopswords_removed]\n",
        "\n",
        "bow = Counter(lemmatized_lokens)\n",
        "\n",
        "print(bow.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPwv5sRRf0Kq",
        "outputId": "2e8fcded-df70-4db9-945d-a16e5776c48f"
      },
      "id": "gPwv5sRRf0Kq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('In', 2), ('grammar', 2), ('linguistics', 1), ('sentence', 1), ('linguistic', 1), ('expression', 1), ('English', 1), ('example', 1), ('The', 1), ('quick', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign Integar Value to Tokens\n",
        "using gensim to make corpus and beg-of-words"
      ],
      "metadata": {
        "id": "XKhOXSOftUk6"
      },
      "id": "XKhOXSOftUk6"
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "paragraph = 'In linguistics and grammar, a sentence is a linguistic expression,sentence such as the English example \"The quick brown fox jumps over the lazy dog.\" In traditional grammar, it is typically defined as a string of words that expresses a complete thought, or as a unit consisting of a subject and predicate.'\n",
        "tokens = [t for t in word_tokenize(paragraph)]\n",
        "alpha_only_tokens = [[t for t in tokens if t.lower().isalpha()]]\n",
        "dictionary = Dictionary(alpha_only_tokens)\n",
        "dictionary.values"
      ],
      "metadata": {
        "id": "4JvIjknBh5qL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8ce74f-e289-4e97-d40d-add2e2abe4d4"
      },
      "id": "4JvIjknBh5qL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Mapping.values of <gensim.corpora.dictionary.Dictionary object at 0x7fa13f897520>>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate bag of words from corpus "
      ],
      "metadata": {
        "id": "9YBOjegrvEA4"
      },
      "id": "9YBOjegrvEA4"
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for i in alpha_only_tokens:\n",
        "  for j in dictionary.doc2bow(i):\n",
        "    print('id: ', dictionary.doc2bow(i)[counter][1], 'value: ', dictionary.get(dictionary.doc2bow(i)[counter][1]))\n",
        "    counter+=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS22eoxzwL7W",
        "outputId": "df618c53-73c3-4f4f-a18e-f94019ee58ca"
      },
      "id": "ZS22eoxzwL7W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id:  1 value:  In\n",
            "id:  2 value:  The\n",
            "id:  1 value:  In\n",
            "id:  6 value:  brown\n",
            "id:  2 value:  The\n",
            "id:  3 value:  a\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  2 value:  The\n",
            "id:  2 value:  The\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  2 value:  The\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  2 value:  The\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  2 value:  The\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n",
            "id:  1 value:  In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_only_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHkbB98Ot8l_",
        "outputId": "6724b0bd-8c6c-4640-e02c-82873c93f592"
      },
      "id": "QHkbB98Ot8l_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['In',\n",
              "  'linguistics',\n",
              "  'and',\n",
              "  'grammar',\n",
              "  'a',\n",
              "  'sentence',\n",
              "  'is',\n",
              "  'a',\n",
              "  'linguistic',\n",
              "  'expression',\n",
              "  'sentence',\n",
              "  'such',\n",
              "  'as',\n",
              "  'the',\n",
              "  'English',\n",
              "  'example',\n",
              "  'The',\n",
              "  'quick',\n",
              "  'brown',\n",
              "  'fox',\n",
              "  'jumps',\n",
              "  'over',\n",
              "  'the',\n",
              "  'lazy',\n",
              "  'dog',\n",
              "  'In',\n",
              "  'traditional',\n",
              "  'grammar',\n",
              "  'it',\n",
              "  'is',\n",
              "  'typically',\n",
              "  'defined',\n",
              "  'as',\n",
              "  'a',\n",
              "  'string',\n",
              "  'of',\n",
              "  'words',\n",
              "  'that',\n",
              "  'expresses',\n",
              "  'a',\n",
              "  'complete',\n",
              "  'thought',\n",
              "  'or',\n",
              "  'as',\n",
              "  'a',\n",
              "  'unit',\n",
              "  'consisting',\n",
              "  'of',\n",
              "  'a',\n",
              "  'subject',\n",
              "  'and',\n",
              "  'predicate']]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name Entity Recognition"
      ],
      "metadata": {
        "id": "DnbndoXyQh-2"
      },
      "id": "DnbndoXyQh-2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Using article on amazon\n",
        "article  = \"Amazon (Amazon.com) is the world's largest online retailer and a prominent cloud service provider. Originally started as an online bookselling company, Amazon has morphed into an internet-based business enterprise that is largely focused on providing e-commerce, cloud computing, digital streaming and artificial intelligence (AI) services. Following an Amazon-to-buyer sales approach, the company offers a monumental product range and inventory, enabling consumers to buy just about anything, including clothing, beauty supplies, gourmet food, jewelry, books, movies, electronics, pet supplies, furniture, toys, garden supplies and household goods. Headquartered in Seattle, Amazon has individual websites, software development centers, customer service centers, data centers and fulfillment centers around the world. History and timeline of Amazon Amazon has come a long way since it was founded by Jeff Bezos in his garage in Bellevue, Wash., on July 5, 1994. The following is a brief history and timeline of events that have evolved Amazon from its humble beginnings to a multinational business empire. The 1990s Amazon officially opened for business as an online bookseller on July 16, 1995. Originally, Bezos had incorporated the company as Cadabra but later changed the name to Amazon. Bezos is said to have browsed a dictionary for a word beginning with A for the value of alphabetic placement. He selected the name Amazon because it was exotic and different and as a reference to his plan for the company's size to reflect that of the Amazon River, one of the largest rivers in the world. Since its inception, the company's motto has always been 'get big fast.' The 2000s In 2005, Amazon Prime This membership-based service for Amazon customers offers free two-day shipping within the contiguous U.S., as well as streaming, shopping and reading benefits. According to Amazon's website, current Amazon Prime membership rates are $14.99 a month or $139 per year. Amazon Web Services This comprehensive and evolving cloud computing platform was also born in the 2000s. The first Amazon Web Services (AWS) offerings were launched in 2006 to provide online services for websites and client-side applications. Amazon Elastic Compute Cloud (EC2) and Simple Storage Service (S3) are the backbones of the company's growing collection of web services. The same year, Amazon also launched a cloud computing and video-on-demand service known at the time as Unbox. By changing the way people bought books, Amazon also shaped how they read them with the launch of its first Kindle e-reader in 2007. This device helps users browse, buy and read e-books, magazines and newspapers from the Kindle Store. From the 2010s to present Amazon debuted its first tablet computer, the Kindle Fire, in 2011 and the Amazon Fire TV Stick, which is part of Amazon's extensive line of streaming media devices, in 2014. Amazon also started an online Amazon Art marketplace for fine arts in 2013, which has featured original works by famous artists such as Claude Monet and Norman Rockwell. The popular in-home virtual assistant Amazon Alexa was rolled out to consumers in 2015 and was followed by the Alexa-equipped Echo Dot in 2016. Amazon acquired the organic grocery store Whole Foods in 2017 and launched Amazon Go, a chain of cashierless grocery stores in 2018. The rise of in-home shopping during the COVID-19 pandemic made consumers rely on Amazon even more, and the trend is likely to keep growing.\"\n"
      ],
      "metadata": {
        "id": "YpnGmqEMNBgD"
      },
      "id": "YpnGmqEMNBgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Break paragraph into sentences "
      ],
      "metadata": {
        "id": "_LMaOPoc008z"
      },
      "id": "_LMaOPoc008z"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "fwiyFUq90lFZ"
      },
      "id": "fwiyFUq90lFZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(article)\n",
        "sentences[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ixv39uczhTl",
        "outputId": "eb1edd37-164b-4d15-929d-554b24d692de"
      },
      "id": "8ixv39uczhTl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Amazon (Amazon.com) is the world's largest online retailer and a prominent cloud service provider.\",\n",
              " 'Originally started as an online bookselling company, Amazon has morphed into an internet-based business enterprise that is largely focused on providing e-commerce, cloud computing, digital streaming and artificial intelligence (AI) services.',\n",
              " 'Following an Amazon-to-buyer sales approach, the company offers a monumental product range and inventory, enabling consumers to buy just about anything, including clothing, beauty supplies, gourmet food, jewelry, books, movies, electronics, pet supplies, furniture, toys, garden supplies and household goods.',\n",
              " 'Headquartered in Seattle, Amazon has individual websites, software development centers, customer service centers, data centers and fulfillment centers around the world.',\n",
              " 'History and timeline of Amazon Amazon has come a long way since it was founded by Jeff Bezos in his garage in Bellevue, Wash., on July 5, 1994.']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Break sentences into tokens"
      ],
      "metadata": {
        "id": "BDumOfS51Lxh"
      },
      "id": "BDumOfS51Lxh"
    },
    {
      "cell_type": "code",
      "source": [
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "token_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKt_UIqI0sj4",
        "outputId": "a0426c75-33aa-400d-d6df-787db101b276"
      },
      "id": "GKt_UIqI0sj4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Amazon',\n",
              " '(',\n",
              " 'Amazon.com',\n",
              " ')',\n",
              " 'is',\n",
              " 'the',\n",
              " 'world',\n",
              " \"'s\",\n",
              " 'largest',\n",
              " 'online',\n",
              " 'retailer',\n",
              " 'and',\n",
              " 'a',\n",
              " 'prominent',\n",
              " 'cloud',\n",
              " 'service',\n",
              " 'provider',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tag tokens into parts of speech"
      ],
      "metadata": {
        "id": "nWEVgXzm2aCw"
      },
      "id": "nWEVgXzm2aCw"
    },
    {
      "cell_type": "code",
      "source": [
        "pos_sentences = [pos_tag(sent) for sent in token_sentences] \n",
        "pos_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt32rJ011IT_",
        "outputId": "0319e484-9c12-4088-b306-b69581050c60"
      },
      "id": "zt32rJ011IT_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Amazon', 'NNP'),\n",
              " ('(', '('),\n",
              " ('Amazon.com', 'NNP'),\n",
              " (')', ')'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('world', 'NN'),\n",
              " (\"'s\", 'POS'),\n",
              " ('largest', 'JJS'),\n",
              " ('online', 'NN'),\n",
              " ('retailer', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('a', 'DT'),\n",
              " ('prominent', 'JJ'),\n",
              " ('cloud', 'NN'),\n",
              " ('service', 'NN'),\n",
              " ('provider', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print name entities"
      ],
      "metadata": {
        "id": "Ou4GSO5DMplh"
      },
      "id": "Ou4GSO5DMplh"
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
        "chunked_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVVSQWmY3P3-",
        "outputId": "934dd678-193e-4d28-83cd-3a2a4b463408"
      },
      "id": "KVVSQWmY3P3-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object ParserI.parse_sents.<locals>.<genexpr> at 0x7f992babcc80>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in chunked_sentences:\n",
        "  for chunk in sent:\n",
        "    if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb6itDPZKAHF",
        "outputId": "d0d7c795-7586-4193-ec45-e52e7d76e2b5"
      },
      "id": "vb6itDPZKAHF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(NE Amazon/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Seattle/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE History/NN)\n",
            "(NE Amazon/NNP Amazon/NNP)\n",
            "(NE Jeff/NNP Bezos/NNP)\n",
            "(NE Bellevue/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Bezos/NNP)\n",
            "(NE Cadabra/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Bezos/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Amazon/NNP River/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE U.S./NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Amazon/NNP Web/NNP Services/NNPS)\n",
            "(NE Amazon/NNP Web/NNP Services/NNP)\n",
            "(NE AWS/NNP)\n",
            "(NE Amazon/NNP Elastic/NNP Compute/NNP Cloud/NNP)\n",
            "(NE EC2/NNP)\n",
            "(NE Simple/NNP Storage/NNP Service/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Unbox/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Kindle/NNP)\n",
            "(NE Kindle/NNP Store/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Kindle/NNP Fire/NNP)\n",
            "(NE Amazon/NNP Fire/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Amazon/NNP Art/NNP)\n",
            "(NE Claude/NNP Monet/NNP)\n",
            "(NE Norman/NNP Rockwell/NNP)\n",
            "(NE Amazon/NNP Alexa/NNP)\n",
            "(NE Amazon/NNP)\n",
            "(NE Whole/NNP Foods/NNP)\n",
            "(NE Amazon/NNP Go/NNP)\n",
            "(NE Amazon/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER with SpaCy"
      ],
      "metadata": {
        "id": "9R66R9HIPcNW"
      },
      "id": "9R66R9HIPcNW"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoduhDcTLldO",
        "outputId": "933837b9-35e2-4a60-d678-6ec3361a893e"
      },
      "id": "RoduhDcTLldO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp(article)\n",
        "for ent in document.ents:\n",
        "    print(f'entity: {ent.label_}', f' -- text: {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo43V8VTPfKj",
        "outputId": "d10c4346-1ff0-47e1-df89-404f8b1b36f0"
      },
      "id": "fo43V8VTPfKj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: Amazon.com\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: AI\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: GPE  -- text: Seattle\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: Amazon Amazon\n",
            "entity: PERSON  -- text: Jeff Bezos\n",
            "entity: GPE  -- text: Bellevue\n",
            "entity: GPE  -- text: Wash.\n",
            "entity: DATE  -- text: July 5, 1994\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: DATE  -- text: 1990s\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: DATE  -- text: July 16, 1995\n",
            "entity: PERSON  -- text: Bezos\n",
            "entity: ORG  -- text: Cadabra\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: PERSON  -- text: Bezos\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: DATE  -- text: 2000s\n",
            "entity: DATE  -- text: 2005\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: DATE  -- text: two-day\n",
            "entity: GPE  -- text: U.S.\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: MONEY  -- text: 14.99\n",
            "entity: MONEY  -- text: 139\n",
            "entity: ORG  -- text: Amazon Web Services\n",
            "entity: DATE  -- text: the 2000s\n",
            "entity: ORDINAL  -- text: first\n",
            "entity: ORG  -- text: Amazon Web Services\n",
            "entity: DATE  -- text: 2006\n",
            "entity: ORG  -- text: Amazon Elastic Compute Cloud\n",
            "entity: ORG  -- text: Simple Storage Service\n",
            "entity: DATE  -- text: The same year\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORDINAL  -- text: first\n",
            "entity: PERSON  -- text: Kindle\n",
            "entity: DATE  -- text: 2007\n",
            "entity: FAC  -- text: the Kindle Store\n",
            "entity: DATE  -- text: the 2010s\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORDINAL  -- text: first\n",
            "entity: GPE  -- text: the Kindle Fire\n",
            "entity: DATE  -- text: 2011\n",
            "entity: ORG  -- text: the Amazon Fire TV Stick\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: DATE  -- text: 2014\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: DATE  -- text: 2013\n",
            "entity: PERSON  -- text: Claude Monet\n",
            "entity: PERSON  -- text: Norman Rockwell\n",
            "entity: PERSON  -- text: Amazon Alexa\n",
            "entity: DATE  -- text: 2015\n",
            "entity: ORG  -- text: Alexa\n",
            "entity: PERSON  -- text: Echo Dot\n",
            "entity: DATE  -- text: 2016\n",
            "entity: ORG  -- text: Amazon\n",
            "entity: ORG  -- text: Whole Foods\n",
            "entity: DATE  -- text: 2017\n",
            "entity: ORG  -- text: Amazon Go\n",
            "entity: DATE  -- text: 2018\n",
            "entity: ORG  -- text: COVID-19\n",
            "entity: ORG  -- text: Amazon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2ZIQP1s0vuN"
      },
      "id": "e2ZIQP1s0vuN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5cLWXkyiPwm0"
      },
      "id": "5cLWXkyiPwm0",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data"
      ],
      "metadata": {
        "id": "cQXNntkC1GzC"
      },
      "id": "cQXNntkC1GzC"
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('movie_review_test.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0Tn-vTWY05Do",
        "outputId": "bf56178d-56c4-413c-b9bb-d42cd750800f"
      },
      "id": "0Tn-vTWY05Do",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class                                               text\n",
              "0   Pos   films adapted from comic books have had plent...\n",
              "1   Pos   every now and then a movie comes along from a...\n",
              "2   Pos   you ve got mail works alot better than it des...\n",
              "3   Pos      jaws   is a rare film that grabs your atte...\n",
              "4   Pos   moviemaking is a lot like being the general m..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd5de998-5f33-4694-bbe4-246a71404a41\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pos</td>\n",
              "      <td>films adapted from comic books have had plent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pos</td>\n",
              "      <td>every now and then a movie comes along from a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pos</td>\n",
              "      <td>you ve got mail works alot better than it des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pos</td>\n",
              "      <td>jaws   is a rare film that grabs your atte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pos</td>\n",
              "      <td>moviemaking is a lot like being the general m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd5de998-5f33-4694-bbe4-246a71404a41')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd5de998-5f33-4694-bbe4-246a71404a41 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd5de998-5f33-4694-bbe4-246a71404a41');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "4j-FvY6315Jc"
      },
      "id": "4j-FvY6315Jc"
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = data['text']\n",
        "labels = data['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size = 0.33 , random_state = 53)"
      ],
      "metadata": {
        "id": "sN2hAIo-1ET2"
      },
      "id": "sN2hAIo-1ET2",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features from text data"
      ],
      "metadata": {
        "id": "LzamrNOJ3D5T"
      },
      "id": "LzamrNOJ3D5T"
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "count_vec_train = count_vectorizer.fit_transform(X_train.values)\n",
        "count_text_test = count_vectorizer.transform(X_test.values)\n",
        "print(count_vectorizer.get_feature_names()[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNSFGvv41y5v",
        "outputId": "cf7d08f2-8674-4997-df91-16ebb14901b1"
      },
      "id": "kNSFGvv41y5v",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00', '000', '10', '100', '101', '102', '105', '109', '11', '110']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model"
      ],
      "metadata": {
        "id": "PVHkkd5B4q_j"
      },
      "id": "PVHkkd5B4q_j"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn import metrics  \n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "lomyhlMO33cz"
      },
      "id": "lomyhlMO33cz",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(count_vec_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNM3roTm4t_T",
        "outputId": "9c89356f-b309-4a51-dd49-5564525ad0f4"
      },
      "id": "dNM3roTm4t_T",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model prediction "
      ],
      "metadata": {
        "id": "BIVfVTI85dhH"
      },
      "id": "BIVfVTI85dhH"
    },
    {
      "cell_type": "code",
      "source": [
        "pred = nb_classifier.predict(count_text_test)"
      ],
      "metadata": {
        "id": "aRqzqFKD5NLF"
      },
      "id": "aRqzqFKD5NLF",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnhkys-55lLG"
      },
      "id": "nnhkys-55lLG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}